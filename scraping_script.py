# -*- coding: utf-8 -*-
"""Uyen-scraping data.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/11GiyES4-tFzzKL_OArHgaCpCbAqV-lQM
"""

import requests
from bs4 import BeautifulSoup
import csv
import os
import base64
import re

# Get the absolute path of the script file
script_path = os.path.abspath(__file__)
script_directory = os.path.dirname(script_path)

# Send a GET request to the website and retrieve the HTML content
url = 'https://nhadat24h.net/'
response = requests.get(url)
html_content = response.text

# Create a BeautifulSoup object to parse the HTML content
soup = BeautifulSoup(html_content, 'html.parser')

# Extract links, images, titles, prices, area, and timestamps
data = []
property_elements = soup.find_all('div', class_='dv-item')

for property_element in property_elements:
    # Extract the image URL
    image_element = property_element.find('img', class_='imageThumb1')
    image_url = image_element.get('data-src') if image_element else ''

    link = property_element.find('a').get('href')

    title_element = property_element.find('a').get('title')
    if title_element is not None:
        title = title_element.strip()
    else:
        title = ''

    price_element = property_element.find('label', class_='a-txt-cl1')
    if price_element is not None:
        price = price_element.text.strip()
    else:
        price = ''

    area_element = property_element.find('label', class_='a-txt-cl2')
    if area_element is not None:
        area = area_element.text.strip()
    else:
        area = ''

    timestamp_element = property_element.find('p', class_='time')
    if timestamp_element is not None:
        timestamp = timestamp_element.text.strip()
    else:
        timestamp = ''

    property_data = {
        'Link': link,
        'Image URL': image_url,
        'Title': title,
        'Price': price,
        'Area': area,
        'Timestamp': timestamp
    }
    data.append(property_data)


for property_data in data:
    print(f"Title: {property_data['Title']}")
    print(f"Price: {property_data['Price']}")
    print(f"Area: {property_data['Area']}")
    print(f"Timestamp: {property_data['Timestamp']}")
    print(f"Link: {property_data['Link']}")
    print()

# Create a directory to store the downloaded images
images_directory = os.path.join(script_directory, 'images')
os.makedirs(images_directory, exist_ok=True)

# Download the images
for i, property_data in enumerate(data):
    image_url = property_data['Image URL']
    title = property_data['Title']
    
    # Replace invalid characters in the title
    title = re.sub(r'[\/:*?"<>|]', '_', title)

    if image_url and image_url.startswith('data:image'):  # Check if it's a base64-encoded image
        # Extract the base64 data part
        encoded_image = image_url.split(',', 1)[1]

        # Decode the base64 data
        image_content = base64.b64decode(encoded_image)

        filename = f'images/{title}.jpg'  # Modify the filename as needed
        with open(filename, 'wb') as file:
            file.write(image_content)
    elif image_url:  # Check if it's a regular image URL
        response = requests.get(image_url)
        image_content = response.content

        filename = os.path.join(images_directory, f'{title}.jpg')  # Modify the filename as needed
        with open(filename, 'wb') as file:
            file.write(image_content)
    elif image_url:  # Check if it's a regular image URL
        response = requests.get(image_url)
        image_content = response.content

        filename =os.path.join(images_directory, f'{title}.jpg')  # Modify the filename as needed
        with open(filename, 'wb') as file:
            file.write(image_content)

# Specify the CSV file path
csv_file = os.path.join(script_directory, 'scraped_data.csv')
# Write the data to CSV
with open(csv_file, 'w', newline='', encoding='utf-8') as file:
    fieldnames = ['Link', 'Image URL', 'Title', 'Price', 'Area', 'Timestamp']
    writer = csv.DictWriter(file, fieldnames=fieldnames)
    writer.writeheader() # Write the header row
    writer.writerows(data) # Write the data rows

print(f'Scraped data and images are saved successfully.')

